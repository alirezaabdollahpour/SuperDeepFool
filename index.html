<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SuperDeepFool: ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Revisiting DeepFool: generalization and improvement</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Revisiting DeepFool: generalization and improvement</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">
              <a href="linkedin.com/in/alireza-abdollahpourorstam-511a36191">Alireza Abdollahpourrostam</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ch/citations?hl=en&user=qosS83IAAAAJ&view_op=list_works&sortby=pubdate">Seyed-Mohsen Moosavi-Dezfooli</a><sup>2</sup>,
            <span class="author-block">
              <a href="https://www.linkedin.com/in/mahed-abroshan/?originalSubdomain=uk">Mahed Abroshan</a><sup>3</sup>,
            </span>
          </div>

          <div class="is-size-3 publication-authors">
            <span class="author-block"><sup>1</sup>Tehran Polytechnic,</span>
            <span class="author-block"><sup>2</sup>Imperial College London,</span>
            <span class="author-block"><sup>3</sup>Optum Labs</span>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div class="column has-text-centered">
      <span class="link-block">
        <a href="https://github.com/alirezaabdollahpour/SuperDeepFool/tree/main"
           class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
              <i class="fab fa-github"></i>
          </span>
          <span>Code</span>
          </a>
      </span>
    </div>
</section>

<!-- <section class="section">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <div style="text-align:center;">
            <img src="https://github.com/alirezaabdollahpour/SuperDeepFool/blob/gh-pages/images/oghab.png" alt="Oghab Image" style="max-width:100%;">
          </div>
        </div>
      </div>
    </div>
</section> -->
  


<section class="section">
  <div class="container">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>Deep neural networks have been known to be vulnerable
            to adversarial examples, which are inputs that are modified
            slightly to fool the network into making incorrect predictions.
            This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the
            robustness to minimal ℓ2 adversarial perturbations. However, existing methods for evaluating this robustness metric
            are either computationally expensive or not very accurate. In
            this paper, we introduce a new family of adversarial attacks
            that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations
            of the well-known DeepFool (DF) attack, while they remain
            simple to understand and implement. We demonstrate that
            our attacks outperform existing methods in terms of both
            effectiveness and computational efficiency. Our proposed
            attacks are also suitable for evaluating the robustness of
            large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal ℓ2
            adversarial perturbations
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
</body>
</html>
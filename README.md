# Revisiting DeepFool: generalization and improvement

Official PyTorch implementation of " Revisiting DeepFool: generalization and improvement "

<p align="center">
<img src="https://github.com/alirezaabdollahpour/SuperDeepFool/blob/main/images/oghab.png" alt="Demo" height="350" width="750"/>
</p>

# Abstract
Deep neural networks have been known to be vulnerable
to adversarial examples, which are inputs that are modified
slightly to fool the network into making incorrect predictions.
This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the
robustness to minimal ℓ2 adversarial perturbations. However, existing methods for evaluating this robustness metric
are either computationally expensive or not very accurate. In
this paper, we introduce a new family of adversarial attacks
that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations
of the well-known DeepFool (DF) attack, while they remain
simple to understand and implement. We demonstrate that
our attacks outperform existing methods in terms of both
effectiveness and computational efficiency. Our proposed
attacks are also suitable for evaluating the robustness of
large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal ℓ2
adversarial perturbations.

# Illustration of SuperDeepFool
<p align="center">
<img src="https://github.com/alirezaabdollahpour/SuperDeepFool/blob/main/images/illus.png" alt="illus" height="300" width="300"/>
</p>

# Running in Docker <img src="https://github.com/alirezaabdollahpour/SuperDeepFool/blob/main/images/docker.png" alt="docker" height="300" width="300"/>
